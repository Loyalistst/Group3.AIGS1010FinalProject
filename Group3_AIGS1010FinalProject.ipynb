{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Downloading Dataset and creation of directory \"Dataset\""
      ],
      "metadata": {
        "id": "i4J8Ed0GSlNm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hKElr9gjX4lm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5df33314-8e83-4428-9b7a-6d3d0ffb52cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "replace dataset/subsample/like/000484ab-5fd0-49b8-9253-23a22b71d7b1.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ],
      "source": [
        "import urllib\n",
        "\n",
        "urllib.request.urlretrieve('https://sc.link/AO5l', 'subsample.zip')\n",
        "!mkdir -p dataset/\n",
        "!unzip -q subsample.zip -d dataset/subsample\n",
        "!rm -r subsample.zip\n",
        "\n",
        "#!scp -r /kaggle/input/hagrid/ann_subsample /kaggle/working/dataset/ann_subsample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4P8xsJ8SukAR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8dc6178-5547-4a50-9740-1bdb0e4e89d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "replace dataset/ann_subsample/ann_subsample/call.json? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ],
      "source": [
        "!unzip -q dataset/ann_subsample.zip -d dataset/ann_subsample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "STmbVU-orEr0"
      },
      "outputs": [],
      "source": [
        "!pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oLWnpq_PtrrI"
      },
      "outputs": [],
      "source": [
        "!ls -lrt dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nLJKexrBvqKT"
      },
      "outputs": [],
      "source": [
        "!ls -lrt dataset/ann_subsample"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing Required Libraries"
      ],
      "metadata": {
        "id": "yBqvB78uYw6z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JC8xeG2mYUHh"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import logging\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "from collections import defaultdict\n",
        "from typing import Tuple\n",
        "from glob import glob\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from PIL import Image, ImageOps\n",
        "import os\n",
        "from ipywidgets import interact\n",
        "from IPython.display import Image as DImage\n",
        "import cv2\n",
        "\n",
        "import torch\n",
        "from torch import nn, Tensor\n",
        "from torchvision import models\n",
        "from torchvision.transforms import Compose\n",
        "from torchvision.transforms import functional as F\n",
        "from torchvision import transforms as T\n",
        "\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fR1ZX4gyxvXO"
      },
      "outputs": [],
      "source": [
        "!pip install torchmetrics\n",
        "from torchmetrics.detection.mean_ap import MeanAveragePrecision"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defining Class names and Formats for gesture recognition dataset -  The class names will be used as labels for each image, while the formats could be used to filter out any irrelevant or incompatible images from the dataset."
      ],
      "metadata": {
        "id": "SU5udTnTZJZ-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bbd6kbuoYUVY"
      },
      "outputs": [],
      "source": [
        "class_names = [\n",
        "   'call',\n",
        "   'dislike',\n",
        "   'fist',\n",
        "   'four',\n",
        "   'like',\n",
        "   'mute',\n",
        "   'ok',\n",
        "   'one',\n",
        "   'palm',\n",
        "   'peace_inverted',\n",
        "   'peace',\n",
        "   'rock',\n",
        "   'stop_inverted',\n",
        "   'stop',\n",
        "   'three',\n",
        "   'three2',\n",
        "   'two_up',\n",
        "   'two_up_inverted',\n",
        "   'no_gesture']\n",
        "\n",
        "FORMATS = (\".jpeg\", \".jpg\", \".jp2\", \".png\", \".tiff\", \".jfif\", \".bmp\", \".webp\", \".heic\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To define functions called \"__get_files_from_dir\" for getting all files in a directory with a given extension and \"__read_annotations\" to read JSON files containing annotations for the dataset"
      ],
      "metadata": {
        "id": "DJeMu4NeZX7d"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hp0p5ebNYkQn"
      },
      "outputs": [],
      "source": [
        "transform = T.ToTensor()\n",
        "\n",
        "class GestureDataset(torch.utils.data.Dataset):\n",
        "\n",
        "    @staticmethod\n",
        "    def __get_files_from_dir(pth: str, extns: Tuple):\n",
        "        if not os.path.exists(pth):\n",
        "            print(f\"Dataset directory doesn't exist {pth}\")\n",
        "            return []\n",
        "        files = [f for f in os.listdir(pth) if f.endswith(extns)]\n",
        "        return files\n",
        "\n",
        "    def __read_annotations(self, path):\n",
        "        annotations_all = None\n",
        "        exists_images = []\n",
        "        for target in class_names:\n",
        "            path_to_csv = os.path.join(path, f\"{target}.json\")\n",
        "            if os.path.exists(path_to_csv):\n",
        "                json_annotation = json.load(open(\n",
        "                    os.path.join(path, f\"{target}.json\")\n",
        "                ))\n",
        "                json_annotation = [dict(annotation, **{\"name\": f\"{name}.jpg\"}) for name, annotation in\n",
        "                                   zip(json_annotation, json_annotation.values())]\n",
        "\n",
        "                annotation = pd.DataFrame(json_annotation)\n",
        "\n",
        "                annotation[\"target\"] = target\n",
        "                annotations_all = pd.concat([annotations_all, annotation], ignore_index=True)\n",
        "                exists_images.extend(\n",
        "                    self.__get_files_from_dir(os.path.join(self.path_images, target), FORMATS))\n",
        "            else:\n",
        "                if target != 'no_gesture':\n",
        "                    print(f\"Database for {target} not found\")\n",
        "\n",
        "        annotations_all[\"exists\"] = annotations_all[\"name\"].isin(exists_images)\n",
        "\n",
        "        annotations_all = annotations_all[annotations_all[\"exists\"]]\n",
        "\n",
        "        users = annotations_all[\"user_id\"].unique()\n",
        "        users = sorted(users)\n",
        "        random.Random(42).shuffle(users)\n",
        "        train_users = users[:int(len(users) * 0.8)]\n",
        "        val_users = users[int(len(users) * 0.8):]\n",
        "\n",
        "        annotations_all = annotations_all.copy()\n",
        "\n",
        "        if self.is_train:\n",
        "            annotations_all = annotations_all[annotations_all[\"user_id\"].isin(train_users)]\n",
        "        else:\n",
        "            annotations_all = annotations_all[annotations_all[\"user_id\"].isin(val_users)]\n",
        "\n",
        "        return annotations_all\n",
        "\n",
        "    def __init__(self, path_annotation, path_images, is_train, transform=None):\n",
        "        self.is_train = is_train\n",
        "        self.transform = transform\n",
        "        self.path_annotation = path_annotation\n",
        "        self.path_images = path_images\n",
        "        self.transform = transform\n",
        "        self.labels = {label: num for (label, num) in\n",
        "                       zip(class_names, range(len(class_names)))}\n",
        "        self.annotations = self.__read_annotations(self.path_annotation)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.annotations.shape[0]\n",
        "\n",
        "    def get_sample(self, index: int):\n",
        "        row = self.annotations.iloc[[index]].to_dict('records')[0]\n",
        "        image_pth = os.path.join(self.path_images, row[\"target\"], row[\"name\"])\n",
        "        image = Image.open(image_pth).convert(\"RGB\")\n",
        "\n",
        "        labels = torch.LongTensor([self.labels[label] for label in row[\"labels\"]])\n",
        "\n",
        "        target = {}\n",
        "        width, height = image.size\n",
        "\n",
        "        bboxes = []\n",
        "\n",
        "        for bbox in row[\"bboxes\"]:\n",
        "            x1, y1, w, h = bbox\n",
        "            bbox_abs = [x1 * width, y1 * height, (x1 + w) * width, (y1 + h) * height]\n",
        "            bboxes.append(bbox_abs)\n",
        "        target[\"labels\"] = labels\n",
        "        target[\"boxes\"] = torch.as_tensor(bboxes, dtype=torch.float32)\n",
        "        target[\"orig_size\"] = torch.as_tensor([int(height), int(width)])\n",
        "\n",
        "        return image, target\n",
        "\n",
        "    def __getitem__(self, index: int):\n",
        "        image, target = self.get_sample(index)\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, target\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setting variables and initializing the PyTorch device to either use the GPU if available or the CPU if not. Additionally, it sets the random seed for the PyTorch, NumPy, and Python random number generators to a fixed value of 42 with 15 epochs"
      ],
      "metadata": {
        "id": "lCbSXfrxZfS4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VnnqzvKEYUcP"
      },
      "outputs": [],
      "source": [
        "random_seed = 42\n",
        "num_classes = len(class_names)\n",
        "batch_size = 16\n",
        "num_epoch = 15\n",
        "torch.manual_seed(random_seed)\n",
        "np.random.seed(random_seed)\n",
        "random.seed(random_seed)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "GestureDataset is defined below to load and preproces the dataset."
      ],
      "metadata": {
        "id": "Jnnn3ZkU4wmi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YH-DZtwSYUfe"
      },
      "outputs": [],
      "source": [
        "train_data = GestureDataset(path_images='dataset/subsample',\n",
        "                            path_annotation='dataset/ann_subsample/ann_subsample',\n",
        "                            is_train=True, transform=transform)\n",
        "\n",
        "test_data = GestureDataset(path_images='dataset/subsample',\n",
        "                            path_annotation='dataset/ann_subsample/ann_subsample',\n",
        "                            is_train=False, transform=transform)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To define a function which is used in object detection tasks for grouping data from multiple images into a batch for training or inference"
      ],
      "metadata": {
        "id": "H0vrNQLa6kdZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gdWD_VQNYUic"
      },
      "outputs": [],
      "source": [
        "def collate_fn(batch):\n",
        "    batch_targets = list()\n",
        "    images = list()\n",
        "\n",
        "    for b in batch:\n",
        "        images.append(b[0])\n",
        "        batch_targets.append({\"boxes\": b[1][\"boxes\"],\n",
        "                              \"labels\": b[1][\"labels\"]})\n",
        "    return images, batch_targets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZtyRVIPlYUlS"
      },
      "outputs": [],
      "source": [
        "train_dataloader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,collate_fn=collate_fn, shuffle=True, num_workers=4)\n",
        "test_dataloader = torch.utils.data.DataLoader(test_data, batch_size=batch_size,collate_fn=collate_fn, shuffle=True, num_workers=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Pretrained framework and  model class creation"
      ],
      "metadata": {
        "id": "molLKk_sZut3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NDw9y-yqYUoX"
      },
      "outputs": [],
      "source": [
        "lr = 0.005\n",
        "momentum = 0.9\n",
        "weight_decay = 5e-4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1htxQ4_LYUrW"
      },
      "outputs": [],
      "source": [
        "model = models.detection.ssdlite320_mobilenet_v3_large(num_classes=len(class_names) + 1, pretrained_backbone=True)\n",
        "model.to(device)\n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rMiecezNYUuF"
      },
      "outputs": [],
      "source": [
        "warmup_factor = 1.0 / 1000\n",
        "warmup_iters = min(1000, len(train_data) - 1)\n",
        "\n",
        "lr_scheduler_warmup = torch.optim.lr_scheduler.LinearLR(\n",
        "    optimizer, start_factor=warmup_factor, total_iters=warmup_iters\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function for Mean Average Precision calculation. \n",
        "Mean Average Precision (mAP) is a famous evaluation metric used in object detection and image segmentation tasks in computer vision. \n",
        "\n",
        "This is widely used to  measure the accuracy of the model in terms of precision and recall. It estimates the average precision (AP) for each class of object detected, and then takes the mean over all the classes to get the final mAP score."
      ],
      "metadata": {
        "id": "SAWoIbdsaH1u"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iilwCHcvYUxP"
      },
      "outputs": [],
      "source": [
        "def eval(model, test_dataloader, epoch):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        mapmetric = MeanAveragePrecision()\n",
        "        \n",
        "        for images, targets in test_dataloader:\n",
        "            images = list(image.to(device) for image in images)\n",
        "            output = model(images)\n",
        "            \n",
        "            for pred in output:\n",
        "                for key, value in pred.items():\n",
        "                    pred[key] = value.cpu()\n",
        "                    \n",
        "            mapmetric.update(output, targets)\n",
        "\n",
        "    metrics = mapmetric.compute()\n",
        "    return metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training loop - training a model using the training dataset, testing the model's performance on the testing dataset after each epoch, and saving the model's state to a file at the end of each epoch"
      ],
      "metadata": {
        "id": "JyYXEyQVarZD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k-PgyZYyYU05"
      },
      "outputs": [],
      "source": [
        "!mkdir checkpoints\n",
        "for epoch in range(num_epoch):\n",
        "    model.train()\n",
        "    total = 0\n",
        "    sum_loss = 0\n",
        "    for images, targets in tqdm(train_dataloader):\n",
        "        batch = len(images)\n",
        "        images = list(image.to(device) for image in images)\n",
        "        for target in targets:\n",
        "            for key, value in target.items():\n",
        "                target[key] = value.to(device)\n",
        "        loss_dict = model(images, targets)\n",
        "        losses = sum(loss for loss in loss_dict.values())\n",
        "        loss = losses.item()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        losses.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        lr_scheduler_warmup.step()\n",
        "\n",
        "        total = total + batch\n",
        "        sum_loss = sum_loss + loss\n",
        "    metrics = eval(model, test_dataloader, epoch)\n",
        "    print(f\"epoch : {epoch}  |||  loss : {sum_loss / total} ||| MAP : {metrics['map']}\")\n",
        "torch.save(model.state_dict(),f\"checkpoints/{epoch}.pth\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creation of a list to store loaded images after using the PIL module"
      ],
      "metadata": {
        "id": "arvqYCdLawjT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5wcuPAdZYU4R"
      },
      "outputs": [],
      "source": [
        "images = []\n",
        "for gesture in class_names[:-1]:\n",
        "    image_path = glob(f'dataset/subsample/{gesture}/*.jpg')[0]\n",
        "    images.append(Image.open(image_path))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creation of a new list of image tensors by applying a PyTorch transform"
      ],
      "metadata": {
        "id": "cBo4G3woDI_T"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m6uWhA_6dlUQ"
      },
      "outputs": [],
      "source": [
        "images_tensors = images.copy()\n",
        "images_tensors_input = list(transform(image).to(device) for image in images_tensors)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inference on the input data by passing it through the specified PyTorch model with gradient calculation disabled, and returning the model's output tensor as out."
      ],
      "metadata": {
        "id": "I7iAhiusEWjF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xlYSKIbLdnp-"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    model.eval()\n",
        "    out = model(images_tensors_input)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To extract the bounding boxes, confidence scores, and class labels for the top two predicted objects in each image from the PyTorch model's output tensor out and to store them in Python lists, which can be used for further processing or visualization of the object detection results."
      ],
      "metadata": {
        "id": "zoxRQRoDa1pD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "saLZAU1_dstk"
      },
      "outputs": [],
      "source": [
        "bboxes = []\n",
        "scores = []\n",
        "labels = []\n",
        "for pred in out:\n",
        "    ids = pred['scores'] >= 0.2\n",
        "    bboxes.append(pred['boxes'][ids][:2].cpu().numpy().astype(np.int))\n",
        "    scores.append(pred['scores'][ids][:2].cpu().numpy())\n",
        "    labels.append(pred['labels'][ids][:2].cpu().numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To create shorter abbreviations for some class names in a list for easier visualization"
      ],
      "metadata": {
        "id": "VIKPP79uFmQX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tRKPyua9dt_k"
      },
      "outputs": [],
      "source": [
        "short_class_names = []\n",
        "\n",
        "for name in class_names:\n",
        "    if name == 'stop_inverted':\n",
        "        short_class_names.append('stop inv.')\n",
        "    elif name == 'peace_inverted':\n",
        "        short_class_names.append('peace inv.')\n",
        "    elif name == 'two_up':\n",
        "        short_class_names.append('two up')\n",
        "    elif name == 'two_up_inverted':\n",
        "        short_class_names.append('two up inv.')\n",
        "    elif name == 'no_gesture':\n",
        "        short_class_names.append('no gesture')\n",
        "    else:\n",
        "        short_class_names.append(name)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To create a list of modified final_images with bounding boxes and adding text labels for the detected objects in the original images"
      ],
      "metadata": {
        "id": "YWzI6vVfGPgy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f7FvHqP7dytX"
      },
      "outputs": [],
      "source": [
        "final_images = []\n",
        "for bbox, score, label, image in zip(bboxes, scores, labels, images):\n",
        "    image = np.array(image)\n",
        "    for i, box in enumerate(bbox):\n",
        "        _,width,_  = image.shape\n",
        "        image = cv2.rectangle(image, box[:2], box[2:], thickness=3, color=[255, 0, 255])\n",
        "        cv2.putText(image, f'{short_class_names[label[i]]}: {score[i]:0.2f}', (box[0], box[1]), cv2.FONT_HERSHEY_SIMPLEX,\n",
        "                        width / 780, (0, 0, 255), 2)\n",
        "    final_images.append(Image.fromarray(image))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To create a new directory named out_images to save each modified image as a PNG file"
      ],
      "metadata": {
        "id": "8KI-9c_uGaU8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tMY3QqCPdzxc"
      },
      "outputs": [],
      "source": [
        "!mkdir out_images\n",
        "out_images = []\n",
        "for i, image in enumerate(final_images):\n",
        "    out_name = f\"out_images/{i}.png\"\n",
        "    out_images.append(out_name)\n",
        "    image.save(out_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Results - To creates a GUI that allows to select an image file from the out_images and displays it"
      ],
      "metadata": {
        "id": "vIOL4vWjbEa5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K3mPVElWd1no"
      },
      "outputs": [],
      "source": [
        "out_dir = \"out_images/\"\n",
        "@interact\n",
        "def show_images(file=os.listdir(out_dir)):\n",
        "    display(DImage(out_dir+file, width=600, height=300))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PROJECT REPORT"
      ],
      "metadata": {
        "id": "cvUj6p3AKPb6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PROJECT TITLE**\n",
        "\n",
        "\"HAND GESTURE RECOGNITION Based on Computer Vision\"\n",
        "\n",
        "**GROUP 3**\n",
        "\n",
        " \tShreya Kiran Bhoir,\n",
        " \tVinay Malik,\n",
        " \tPriyanka Awasthi,\n",
        " \tVignesh Ram Sundararaman,\n",
        " \tVikash Raj Chandrabalu\n",
        "\n",
        "**MENTOR**\n",
        "\n",
        "BABATUNDE GIWA\n",
        "\n",
        "\t\n",
        "**OBJECTIVE**\n",
        "\n",
        "To build hand gesture recognition (HGR) system, which can be used in video conferencing services, home automation systems, or in the automotive sector.\n",
        "\n",
        "**PROJECT SUMMARY**\n",
        "\n",
        "Hand gesture recognition is an application of computer vision concepts for detecting and interpreting the movements or positions of a hand or fingers. The project's goal is to build a hand gesture recognition system. \n",
        "\n",
        "**SCOPE AND ALGORITHM**\n",
        "\n",
        "The hand gesture recognition project entails creating a system that can detect various hand gestures. Deep learning-based methods have recently become an effective approach to hand gesture recognition due to the availability of annotated datasets and advances in hardware and software technologies. Deep neural networks are used in these methods to learn the features and the classifier from the input data at the same time. Convolutional neural networks (CNNs) are the most used architecture in this field, and they have shown promising results on several benchmark datasets.\n",
        "\n",
        "**TOOLS AND REQUIREMENTS**\n",
        "\n",
        "•\tPython \n",
        "•\tJupyter Notebook\n",
        "•\tPandas\n",
        "•\tNumpy\n",
        "•\tMatplotlib\n",
        "•\tPyTorch\n",
        "\n",
        "**METHODOLOGY**\n",
        "\n",
        "DATA COLLECTION:\n",
        "\n",
        "We used Kaggle dataset - a large image dataset HaGRID (HAnd Gesture Recognition Image Dataset) which is publicly available, to test our system. The dataset is collected into our local machines and the files are imported in python.\n",
        "HaGRID is 716GB in size, and the dataset contains 552,992 FullHD (1920 1080) RGB images divided into 18 gesture classes. In addition, if there is a second free hand in the frame, some images have the no gesture class. There are 123,589 samples in this extra class. By subject user-id, the data was divided into 92% training and 8% testing sets, with 509,323 images for train and 43,669 images for test.\n",
        "\n",
        "MODEL:\n",
        "\n",
        "To implement a hand gesture recognition system using computer vision techniques, the following methodology can be followed:\n",
        "Extraction of features from the preprocessed data and then training a machine learning model using the extracted features followed by testing the model on a separate set of hand gesture data and at last evaluating the performance of the model.\n",
        "\n",
        "INTERPRETATION OF RESULTS:\n",
        "\n",
        "The results of the hand gesture recognition system can be interpreted in terms of accuracy and precision. Accuracy measures the percentage of correct predictions made by the model; precision measures the percentage of true positive predictions out of all positive predictions. A higher accuracy and precision indicate better performance of the system.\n",
        "\n",
        "The data was then divided into training and testing sets using NumPy array slicing. The training and testing data are reshaped to be compatible with a model.\n",
        "\n",
        "We compiled the model with the a optimizer and the sparse categorical crossentropy loss function, then train it for 15 epochs on the training set.\n",
        "We evaluated the model on the testing set using the evaluate() function, and printed the test accuracy. Mean Average Precision (mAP) function, a famous evaluation metric, is used in object detection and image segmentation tasks.\n",
        "This is widely used to measure the accuracy of the model in terms of precision and recall. It estimates the average precision (AP) for each class of object detected, and then takes the mean over all the classes to get the final mAP score.\n",
        "\n",
        "The MAP score in object detection measures how accurately an algorithm localises objects of interest and distinguishes them from other objects in the image. The score is calculated by averaging the precision-recall curves for each object class, where precision is the ratio of true positives to predicted positives and recall is the ratio of true positives to ground-truth positives.\n",
        "A high MAP score indicates that the algorithm can retrieve relevant images with a high degree of precision and recall.\n",
        "\n",
        "**COMPUTER VISION CONCEPTS USED IN HAND GESTURE RECOGNITION:**\n",
        "\n",
        "Here are some computer vision terms commonly used in hand gesture recognition:\n",
        "Hand detection: The process of identifying and localizing the hand in an image.\n",
        "Hand segmentation: The process of separating the hand region from the background in an image or video frame.\n",
        "\n",
        "Feature extraction: The process of identifying key characteristics or features of the hand, such as its shape, size, and position.\n",
        "\n",
        "Image preprocessing: Techniques used to enhance the input image.\n",
        "Image segmentation: Techniques used to separate the hand from the background.\n",
        "Feature extraction: Techniques used to extract relevant information from the preprocessed image.\n",
        "Machine learning: Techniques used to train a model to classify the hand gesture based on the extracted features.\n",
        "Classification: The process of assigning a specific gesture or action to the detected hand movements.\n",
        "Deep learning: A machine learning approach that uses artificial neural networks to learn from data, which is commonly used in hand gesture recognition systems.\n",
        "Gesture recognition: The process of identifying and interpreting specific hand gestures, such as pointing, waving, or making a fist.\n",
        "\n",
        "**TECHNICAL ANALYSIS**\n",
        "\n",
        "The hand gesture recognition system may be technically examined using computer vision techniques such as image processing, feature extraction, and machine learning. Image processing techniques such as noise reduction and image segmentation can be used to improve the quality of the input image. Relevant information from the preprocessed picture, such as the shape and location of the hand, may be extracted using feature extraction algorithms. Machine learning techniques may be used to train a model to categorise the hand gesture based on the retrieved attributes.\n",
        "\n",
        "**LITERATURE REVIEW**\n",
        "\n",
        "1)\thttps://www.ncbi.nlm.nih.gov/pmc/articles/PMC8321080/#:~:text=Algorithms%20have%20been%20developed%20based,deep%20learn%20detection%20and%20more.\n",
        "\n",
        "Munir Oudah,1 Ali Al-Naji,1 and Javaan Chahl2, “Hand Gesture Recognition Based on Computer Vision: A Review of Techniques” Electrical Engineering Technical College, Middle Technical University, J Imaging. 2020 August\n",
        "\n",
        "Hand gestures are a type of nonverbal communication that can be used in a variety of fields, including deaf-mute communication, robot control, human-computer interaction (HCI), home automation, and medical applications. Many different techniques have been used on hand gestures, including those based on instrumented sensor technology and computer vision. In other words, the hand sign can be divided into several categories, including posture and gesture, as well as dynamic and static, or a hybrid of the two. This paper focuses on a review on hand gesture techniques and introduces their benefits and drawbacks in various situations. Furthermore, it tabulates the performance of these methods, with a focus on computer vision techniques dealing with similarity and difference points.\n",
        "\n",
        "2) https://www.ijert.org/research/dynamic-hand-gesture-recognition-a-literature-review-IJERTV1IS9222.pdf\n",
        "\n",
        "Deepali N. Kakade, Prof. Dr. J.S. Chitode, “Dynamic Hand Gesture Recognition”, “International Journal of Engineering Research & Technology (IJERT)”, Vol. 1 Issue 9, November- 2012\n",
        "\n",
        "This paper reviews recent hand gesture recognition systems which have gained attention due to their ability to efficiently interact with computer systems through human-computer interaction. The paper demonstrated how to create a natural interface between humans and computers by recognizing gestures for controlling robots or conveying information. The paper covers camera interfaces, image processing, hand gestures, color detection, and recognition. The advantages of using hand gestures include ease of use, naturalness, and intuitiveness, which have made them successful in applications such as computer game control, human-robot interaction, and sign language recognition. In the past, glove-based devices were used, but they were cumbersome and unnatural. Video-based non-contact interaction techniques have made gesture inputs more natural and improved the interface between humans and computers. \n",
        "\n",
        "3)\thttps://web.stanford.edu/class/cs231a/prev_projects_2016/CS231A_Project_Final.pdf\n",
        "\n",
        "Zi Xian, Justin Yeo, “Hand Recognition and Gesture Control Using a Laptop Web-camera” Stanford University 450 Serra Mall, Stanford\n",
        "\n",
        "Given the recent growth and popularity of Virtual and Augmented Reality, hand gesture recognition is a technology that is becoming increasingly important. It is an important aspect of Human Computer Interaction (HCI) because it allows for two-way interaction in virtual spaces. Many examples of such interaction, however, are currently limited to specialised applications or more expensive devices such as the Kinect and the Oculus Rift. In this paper, they investigated hand gesture recognition methods using a more common device - a laptop webcam. They focused three different methods of segmenting the hand, documenting the advantages and disadvantages of each method.\n",
        "\n",
        "**CONCLUSION**\n",
        "\n",
        "Our hand gesture recognition project results show that we achieved a test accuracy of 99.05%. This demonstrates that our model can recognise hand gestures accurately in real-time using a webcam and can be used in a variety of applications such as human-computer interaction and robotics. It is worth noting, however, that our results were obtained using a controlled dataset and that additional testing on diverse datasets may be required to evaluate the model's generalisation performance.\n",
        "\n",
        "**REFERENCES AND CITATION:**\n",
        "\n",
        "https://www.researchgate.net/publication/284626785_Hand_Gesture_Recognition_A_Literature_Review\n",
        "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8321080/#:~:text=Algorithms%20have%20been%20developed%20based,deep%20learn%20detection%20and%20more.\n",
        "https://www.mdpi.com/2313-433X/6/8/73\n",
        "https://www.academia.edu/17775220/Hand_Gesture_Recognition_A_Literature_Review\n",
        "https://www.researchgate.net/publication/284626785_Hand_Gesture_Recognition_A_Literature_Review\n",
        "https://www.kaggle.com/code/adinishad/hand-sign-recognition-cnn-keras-97-accuracy?scriptVersionId=41482305&cellId=4\n",
        "https://web.stanford.edu/class/cs231a/prev_projects_2016/CS231A_Project_Final.pdf\n",
        "https://www.mdpi.com/2313-433X/6/8/73\n",
        "https://peerj.com/articles/cs-218/\n",
        "https://gitlab.aicloud.sbercloud.ru/rndcv/hagrid\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "NF1GbzUDKEDm"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "N9DcEJqHJivV"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}